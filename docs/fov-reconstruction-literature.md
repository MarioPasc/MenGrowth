### Tabla de métodos generativos para FOV-extension / crop filling en brain MRI

| Artículo (1er autor, año)                                                                                                                                                                                                   | Tipo de método generativo (resumen)                                                                                                                                                                                                                                                                                                            | Modalidades MRI y tipo de recorte                                                                                                                                                                                                     | Encaje con tu estudio (T1c+T1n+T2w+T2-FLAIR, 1 mm³)                                                                                                                                                                                                                                                                                         |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Gao et al., 2024 – “Field-of-View Extension for Brain Diffusion MRI via Deep Generative Models” (J Med Imaging)** ([arXiv][1])                                                                                            | Framework GAN-like 3D: generador y discriminador; aprende a imputar las *slices* ausentes en dMRI usando como entrada el DWI truncado más la anatomía T1w correspondiente.                                                                                                                                                                     | dMRI (DWI b0 y b=1300) condicionado en T1w; FOV incompleto típicamente inferior/superior; resolución ~1.25 mm isotrópica. ([ar5iv][2])                                                                                                | Encaje conceptual alto (misma formulación de “missing superior slices”), pero modalmente está centrado en dMRI. Para tu pipeline estructural multi-contraste tendrías que adaptar el generador a canales T1c/T1n/T2w/T2-FLAIR y redefinir la función de pérdida para anatomía cortical, no para tractografía.                               |
| **Li et al., 2024 – “Multi-Modality Conditioned Variational U-Net for Field-of-View Extension in Brain Diffusion MRI” (arXiv:2409.13846)** ([arXiv][3])                                                                     | VAE-U-Net (Variational U-Net, CVAE): codifica DWI parcial y T1w completo, y decodifica las regiones ausentes de dMRI. Explota explícitamente el condicionamiento multimodal en T1.                                                                                                                                                             | dMRI + T1w estructural; problema de FOV incompleto en dMRI con tractografía como tarea downstream.                                                                                                                                    | De nuevo, el encaje es conceptual (modalidad condicionante + modalidad objetivo recortada) pero no directo: está pensado para dMRI. Es un muy buen “blueprint” arquitectónico si quieres un modelo que use tus 4 contrastes como condición para rellenar un contraste objetivo (p.ej. T1c) recortado.                                       |
| **Ayub et al., 2020/2021 – “Inpainting Cropped Diffusion MRI Using Deep Generative Models” (MICCAI workshop DGM4MICCAI)** ([PMC][4])                                                                                        | Comparan VAE, VQVAE, VAE-GAN y proponen U-VQVAE (U-Net 3D con “vector quantization” en el bottleneck) para inpainting. Modelo explícitamente generativo, entrenado a reconstruir la parte superior truncada del cerebro en dMRI.                                                                                                               | dMRI (DWI) con parte superior del cerebro recortada; usan datos NCANDA; reconstruyen las slices faltantes en espacio de voxel.                                                                                                        | Muy buena referencia metodológica sobre cómo formular el problema y evaluar imputación en FOV recortado. Igual que los anteriores, está centrado en dMRI; para tu caso necesitarías re-entrenar con estructurales multi-contraste y quizá usar canales multi-modales de entrada.                                                            |
| **Tang et al., 2022 – “TW-BAG: Tensor-Wise Brain-Aware Gate Network for DTI Inpainting”** ([arXiv][5])                                                                                                                      | 3D U-Net especializado con “Brain-Aware Gate” (BAG) y decodificadores por-tensor (Dxx…Dzz). No es un modelo probabilístico, sino una CNN determinista de regresión para inpainting de las regiones truncadas en el extremo superior del volumen DTI.                                                                                           | DTI (6 coeficientes tensoriales) con slices superiores recortadas (≈10 % del cerebro). Usa máscara cerebral derivada de T1w como entrada adicional.                                                                                   | Muy alineado con tu geometría de problema (slices faltantes en dirección superior-inferior), pero limitado a DTI. Como patrón de diseño, muestra cómo integrar máscaras anatómicas y explotar 3D completo; podría inspirar una variante multi-canal donde tus secuencias estructurales proporcionen contexto.                               |
| **Torrado-Carvajal et al., 2021 – “Inpainting as a Technique for Estimation of Missing Voxels in Brain Imaging” (Ann Biomed Eng)** ([PubMed][6])                                                                            | Inpainting clásico (no deep) basado en métodos de restauración de imágenes: técnicas patch-based y modelos de interpolación avanzada para rellenar voxels perdidos. Comparan inpainting con interpolaciones estándar en mapas de MRSI (NAA) y en T1w; muestran mejoras claras de NRMSE.                                                        | T1w estructural y mapas de espectroscopia (CSI); voxels perdidos distribuidos de forma aleatoria o en regiones típicamente de baja SNR. No está ligado a un patrón concreto de FOV, pero el formalismo vale para “holes” arbitrarios. | Encaje medio: demuestra que, incluso sin deep learning, el inpainting puede ser superior a la simple interpolación cuando hay voxels perdidos. No explota multi-contraste ni está pensado específicamente para cortes craneales ausentes, pero es una referencia metodológica importante sobre cómo evaluar el “filling” cuantitativamente. |
| **Leal et al., 2024 – “Crop Filling: a pipeline for repairing memory clinic MRI corrupted by partial brain coverage” (MethodsX)** ([UCL Discovery][7])                                                                      | Pipeline basado en *SynthSR* / *SynthSR Hyperfine*: red de síntesis/super-resolución 3D (U-Net-like), entrenada para generar un T1w 1×1×1 mm tipo MPRAGE a partir de T2w (y luego de T1w parcial + T2w). El pipeline rellena la T1w recortada copiando los voxels del T1w sintético donde hay ceros y re-síntetiza una versión refinada.       | T1w estructural (recortada) + T2w clínica; salida final: T1w completa 1 mm³, con FOV extendido a 256³. ([UCL Discovery][7])                                                                                                           | Es, hoy por hoy, el método más directamente aplicable a tu caso: mismo dominio (estructural), misma idea de FOV recortado, y misma resolución 1 mm³. Puedes reinterpretar T1c/T1n/T2w/T2-FLAIR como modalidades de entrada para un modelo tipo SynthSR y adaptar el pipeline de Crop Filling (el código Docker está publicado).             |
| **LaPoint et al., 2022 – “Crop Filling: artefact removal for real-world clinical neuroimaging data” (Alzheimer’s & Dementia, abstract)** ([Alzheimer's & Dementia Journals][8])                                             | Comunicación que introduce el concepto de “Crop Filling” como artefact removal para T1w clínicos; se apoya en SynthSR para generar T1w sintéticas 3D a partir de datos 2D/heterogéneos y reemplazar regiones recortadas.                                                                                                                       | T1w estructural clínico (2D/3D) con cobertura parcial sistemática en memoria clínica; uso indirecto de T2/FLAIR como entrada de SynthSR.                                                                                              | Encaje alto a nivel de *use-case* clínico (datos heterogéneos, FOV truncado). A nivel técnico remite a las mismas herramientas que Leal et al., pero refuerza su validez en entornos clínicos reales.                                                                                                                                       |
| **Iglesias et al., 2021 – “Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast” (NeuroImage)** ([UCL Discovery][7]) | Red generativa 3D tipo U-Net que hace simultáneamente super-resolución e imagen sintética de T1w 1 mm³ estilo MP-RAGE a partir de múltiples secuencias heterogéneas (T1/T2/FLAIR, resoluciones y orientaciones variadas). Es el núcleo de *SynthSR*.                                                                                           | Multicontraste estructural (T1, T2, FLAIR, etc.), entradas 2D/3D en resoluciones variables; salida T1w 1 mm³ isotrópica.                                                                                                              | Es la pieza clave si quieres un modelo generativo multicontraste para tu combinación T1c/T1n/T2w/T2-FLAIR en 1 mm³. No está formulado explícitamente como “FOV extension”, pero en Crop Filling se usa precisamente para reconstruir regiones recortadas, lo cual encaja extremadamente bien con tu escenario.                              |
| **Iglesias et al., 2023 – “SynthSR: a public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry” (Sci Adv)** ([UCL Discovery][7])                                | Publicación de la herramienta *SynthSR*: framework generativo para obtener T1w 1 mm³ de alta calidad a partir de MRI heterogéneas (incluyendo 2D clínicas). Extiende el trabajo anterior y libera código en FreeSurfer.                                                                                                                        | Entradas: múltiples contrastes estructurales (T1/T2/FLAIR) y múltiples protocolos; salida: T1w 1 mm³.                                                                                                                                 | Junto con Leal et al., es la referencia más sólida para justificar un enfoque de “crop filling” multicontraste sustentado en un modelo generativo ya entrenado y mantenido. Para tu estudio puedes usar directamente SynthSR como backbone, con tuning específico si dispones de datos propios.                                             |
| **Chai et al., 2020 – “MRI restoration using edge-guided adversarial learning” (EG-GAN)** ([PMC][9])                                                                                                                        | EG-GAN: GAN 3D con rama de bordes (edge-guided) que separa la recuperación de componentes de alta y baja frecuencia. Diseñada para restaurar el detalle perdido en escaneos 2D clínicos, combinando información de slices adyacentes; explícitamente mencionan aplicaciones a reconstruir datos con artefactos o escaneos 2D para estudios 3D. | Principalmente T1w cerebrales de HCP (0.7 mm) degradados o subsampleados en el eje de slices; restauración 3D desde adquisiciones 2D/semisparse. ([PMC][9])                                                                           | Encaje medio: el problema es muy próximo (faltan cortes o hay resolución muy baja en una dirección). No está escrito como FOV extension pero la arquitectura es un GAN 3D de restauración que podría re-entrenarse con máscaras de FOV truncado en tus volúmenes multi-contraste.                                                           |
| **Zhou et al., 2024 – “Adversarial Learning for MRI Reconstruction and Classification” (incluye GAN-VAN y GAN-NOV)** ([PMC][10])                                                                                            | Proponen dos variantes GAN para imputar slices sagitales faltantes (GAN-VAN, GAN-NOV) en volúmenes T1w 3D, como parte de un pipeline de clasificación de deterioro cognitivo. Las GANs aprenden a “rellenar” slices ausentes en volúmenes “diced” (con cortes aleatoriamente eliminados).                                                      | T1w estructural; slices sagitales eliminadas aleatoriamente (no necesariamente un recorte sistemático superior/inferior).                                                                                                             | Encaje medio: demuestra que GANs 3D pueden rellenar slices enteras y que el imputado puede alimentar un modelo de clasificación clínica. Para tu caso habría que adaptar el patrón de recorte (sistemático craneal) y el condicionamiento multicontraste.                                                                                   |
| **Armanious et al., 2019 – “ipA-MedGAN: Inpainting of Arbitrary Regions in Medical Imaging”** ([loggialab.mgh.harvard.edu][11])                                                                                             | GAN de inpainting 2D para regiones arbitrarias (ipA-MedGAN), con discriminadores global y local. Evaluada en varias modalidades médicas, incluyendo MRI.                                                                                                                                                                                       | MRI 2D (entre otras), con máscaras arbitrarias que simulan regiones faltantes.                                                                                                                                                        | Encaje conceptual: te da un baseline de inpainting GAN bien establecido, pero habría que extenderlo a 3D y a multi-contraste. No trata FOV truncado específicamente ni 1 mm³ isotrópico.                                                                                                                                                    |
| **Biophysically-conditioned generative framework para 3D MRI inpainting (preprint 2025)** ([arXiv][12])                                                                                                                     | Framework de inpainting 3D condicionado por mapas de tejido y concentración tumoral; genera parches anatómicamente plausibles tanto en tejido sano como tumoral. Es un modelo generativo condicionado (no necesariamente GAN; mezcla de redes convolucionales y priors biofísicos).                                                            | T1/T2 estructural con y sin tumor; se centra en inpainting de lesiones/máscaras arbitrarias en todo el volumen.                                                                                                                       | Encaje bajo-medio: interesante por la idea de condicionar el generador en mapas de tejido (puedes imaginar condicionar en mapas de segmento cortical obtenidos de T1c/T2-FLAIR), pero el objetivo no es FOV extension sino lesion inpainting.                                                                                               |

### Comentarios globales

1. **Para FOV extension estructural multicontraste como el tuyo, hoy el “stack” más cercano y reutilizable es:**

   * *SynthSR* / *SynthSR Hyperfine* como backbone generativo 3D multicontraste.([UCL Discovery][7])
   * El pipeline **MRI Crop Filling** de Leal et al. como prueba de concepto de que puedes rellenar T1w recortadas usando T2w y obtener T1w completas 1 mm³.([UCL Discovery][7])

2. **En dMRI** tienes una línea de trabajo madura (Ayub→Gao→Li, más TW-BAG) que formaliza el problema exactamente como el tuyo (slices ausentes en el extremo craneal) pero en espacio de difusión.([arXiv][1])
   Esa literatura es muy útil para:

   * cómo diseñar los máscaras de entrenamiento (qué porcentaje del cerebro recortas, cómo simulas recorte clínico);
   * qué métricas usar (PSNR/SSIM locales, métricas de tractografía o, en tu caso, de segmentación tumoral downstream).

3. **En métodos más generales de inpainting (Torrado-Carvajal, EG-GAN, ipA-MedGAN, Biophysically-conditioned)** tienes ideas transferibles sobre:

   * por qué el inpainting gana a la simple interpolación cuando el “vacío” no contiene información;([PubMed][6])
   * cómo imponer coherencia anatómica de alta frecuencia (bordes, texturas) en la zona reconstruida utilizando pérdidas perceptuales/adversariales.([SciSpace][13])

En conjunto, si quieres algo *directamente aplicable* a T1c+T1n+T2w+T2-FLAIR en 1 mm³ y FOV recortado hacia craneal, lo más pragmático es tomar **SynthSR + Crop Filling** como solución baseline (ya probada clínicamente) y, si quieres ir más lejos, diseñar un **VAE-U-Net o GAN 3D multicanal** al estilo de Li/Gao/Ayub, usando tus 4 contrastes completos/parciales para aprender a reconstruir las slices superiores ausentes en el espacio atlas.

[1]: https://arxiv.org/abs/2405.03652?utm_source=chatgpt.com "Field-of-View Extension for Brain Diffusion MRI via Deep Generative Models"
[2]: https://ar5iv.org/pdf/2405.03652 "[2405.03652] Field-of-View Extension for Brain Diffusion MRI via Deep Generative Models"
[3]: https://arxiv.org/abs/2409.13846?utm_source=chatgpt.com "Multi-Modality Conditioned Variational U-Net for Field-of-View Extension in Brain Diffusion MRI"
[4]: https://pmc.ncbi.nlm.nih.gov/articles/PMC8123091/?utm_source=chatgpt.com "Inpainting Cropped Diffusion MRI using Deep Generative ..."
[5]: https://arxiv.org/pdf/2210.17076?utm_source=chatgpt.com "TW-BAG: Tensor-wise Brain-aware Gate Network for ..."
[6]: https://pubmed.ncbi.nlm.nih.gov/32632531/?utm_source=chatgpt.com "Inpainting as a Technique for Estimation of Missing Voxels in ..."
[7]: https://discovery.ucl.ac.uk/10186845/1/1-s2.0-S2215016123005381-main.pdf "Crop filling: A pipeline for repairing memory clinic MRI corrupted by partial brain coverage"
[8]: https://alz-journals.onlinelibrary.wiley.com/doi/abs/10.1002/alz.063636?utm_source=chatgpt.com "Crop Filling: artefact removal for real‐world clinical ..."
[9]: https://pmc.ncbi.nlm.nih.gov/articles/PMC7977797/?utm_source=chatgpt.com "MRI restoration using edge-guided adversarial learning"
[10]: https://pmc.ncbi.nlm.nih.gov/articles/PMC11326336/?utm_source=chatgpt.com "Adversarial Learning for MRI Reconstruction and ..."
[11]: https://loggialab.mgh.harvard.edu/wp-content/uploads/2023/09/Inpainting-as-a-Technique-for-Estimation-of-Missing-Voxels-in-Brain-Imaging.pdf?utm_source=chatgpt.com "Inpainting as a Technique for Estimation of Missing Voxels ..."
[12]: https://arxiv.org/html/2510.09365v1?utm_source=chatgpt.com "A Biophysically-Conditioned Generative Framework for 3D ..."
[13]: https://scispace.com/pdf/mri-restoration-using-edge-guided-adversarial-learning-4zq1oekkmj.pdf?utm_source=chatgpt.com "MRI restoration using edge-guided adversarial learning."
